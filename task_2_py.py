# -*- coding: utf-8 -*-
"""Task_2.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16t4O6auOsFaLgP1NFNoF-vDeQnhQci8o
"""

# Installing all the necessary Dependencies
!pip install datasets
!pip install fsspec==2023.6.0 --quiet
!pip install -q sentence-transformers
!pip install faiss-cpu
!pip install arize-phoenix-evals transformers accelerate

# Import necessary libraries for data handling, NLP tasks, and lemmatization
import pandas as pd;import nltk,string
from nltk.corpus import stopwords;from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('punkt_tab')

# Load quotes dataset and check basic structure
from datasets import load_dataset

dataset=load_dataset("Abirate/english_quotes")
df=dataset['train'].to_pandas()

print("Shape:",df.shape)
print("Null counts:\n",df.isnull().sum())

# Clean 'quote' and 'author' columns
stop_words=set(stopwords.words('english'))
lemmatizer=WordNetLemmatizer()

df['quote']=df['quote'].str.lower().str.translate(str.maketrans('','',string.punctuation))
df['quote']=df['quote'].apply(word_tokenize)
df['quote']=df['quote'].apply(lambda tokens:[t for t in tokens if t not in stop_words])
df['quote']=df['quote'].apply(lambda tokens:[lemmatizer.lemmatize(t)for t in tokens])
df['quote']=df['quote'].apply(lambda tokens:' '.join(tokens))

df['author']=df['author'].str.lower().str.strip().str.translate(str.maketrans('','',string.punctuation))

df.head()

# Subsample for quick training
df=df.drop_duplicates(subset=['quote']).sample(2000,random_state=42)  # Sample 2,000 unique quotes
print("Subsampled shape:",df.shape)

# Prepare SentenceTransformer fine-tuning
from sentence_transformers import SentenceTransformer,InputExample,losses
from torch.utils.data import DataLoader

train_examples=[InputExample(texts=[q,q])for q in df['quote']]  # Create (quote, quote) pairs
train_dataloader=DataLoader(train_examples,shuffle=True,batch_size=16)

model=SentenceTransformer('all-MiniLM-L6-v2')
train_loss=losses.MultipleNegativesRankingLoss(model)

model.fit(train_objectives=[(train_dataloader,train_loss)],epochs=1,warmup_steps=100,show_progress_bar=True)

# Build corpus embeddings and define semantic search
from sentence_transformers import SentenceTransformer
import torch
from torch.nn.functional import cosine_similarity

model=SentenceTransformer("all-MiniLM-L6-v2")

corpus=df.apply(lambda row:f"{row['quote']} {row['author']} {' '.join(row['tags'])}",axis=1).tolist()
corpus_embeddings=model.encode(corpus,convert_to_tensor=True)
print("Corpus size:",corpus_embeddings.shape[0])

def search_quotes(query,top_k=5):
    query_embedding=model.encode(query,convert_to_tensor=True)
    scores=cosine_similarity(query_embedding.unsqueeze(0),corpus_embeddings).squeeze(0)
    top_results=torch.topk(scores,k=min(top_k,scores.shape[0]))
    results=[]
    for idx,score in zip(top_results.indices,top_results.values):
        i=idx.item()
        results.append({
            "quote":df.iloc[i]['quote'],
            "author":df.iloc[i]['author'],
            "tags":df.iloc[i]['tags'],
            "score":score.item()
        })
    return results

results=search_quotes("hope by Oscar Wilde",top_k=3)
for r in results:
    print(f"{r['quote']} — {r['author']} [score: {r['score']:.4f}]")

# Save fine-tuned model
model.save("fine-tuned-quote-embedder")

# Build FAISS index for fast retrieval
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss

model=SentenceTransformer('fine-tuned-quote-embedder')
corpus=df.apply(lambda row:f"{row['quote']} {row['author']} {' '.join(row['tags'])}",axis=1).tolist()
corpus_embeddings=model.encode(corpus,convert_to_numpy=True,normalize_embeddings=True)

embedding_dim=corpus_embeddings.shape[1]
index=faiss.IndexFlatIP(embedding_dim)
index.add(corpus_embeddings)
print("FAISS index size:",index.ntotal)

def retrieve_with_faiss(query,top_k=5):
    query_vec=model.encode(query,convert_to_numpy=True,normalize_embeddings=True)
    query_vec=np.expand_dims(query_vec,axis=0)
    scores,indices=index.search(query_vec,top_k)
    results=[]
    for s,i in zip(scores[0],indices[0]):
        results.append({
            "quote":df.iloc[i]['quote'],
            "author":df.iloc[i]['author'],
            "tags":df.iloc[i]['tags'],
            "score":float(s)
        })
    return results

results=retrieve_with_faiss("quotes about hope by Oscar Wilde",top_k=3)
for r in results:
    print(f"{r['quote']} — {r['author']} [score: {r['score']:.4f}]")

# Simple RAG example using FLAN-T5
from transformers import AutoTokenizer,AutoModelForSeq2SeqLM,pipeline

model_id="google/flan-t5-base"
tokenizer=AutoTokenizer.from_pretrained(model_id)
model=AutoModelForSeq2SeqLM.from_pretrained(model_id)

rag_pipe=pipeline("text2text-generation",model=model,tokenizer=tokenizer)

prompt=(
    'Use the following quotes to answer the question.\n\n'
    'Quotes:\n'
    '- "hope is a waking dream" — Aristotle\n'
    '- "we must accept finite disappointment, but never lose infinite hope" — Martin Luther King Jr.\n\n'
    'Question: What do famous people say about hope?\n\nAnswer:'
)

print(rag_pipe(prompt,max_new_tokens=100)[0]["generated_text"])

# Build evaluation dataset and evaluate relevance
import pandas as pd
from transformers import AutoTokenizer,AutoModelForSeq2SeqLM
from sklearn.metrics import classification_report

df_eval=pd.DataFrame([
    {"user_query":"quotes about hope by Oscar Wilde",
     "quote":"we are all in the gutter, but some of us are looking at the stars.",
     "author":"oscar wilde",
     "is_relevant":True},
    {"user_query":"quotes about success",
     "quote":"loved poor",
     "author":"oscar wilde",
     "is_relevant":False},
])

tokenizer=AutoTokenizer.from_pretrained(model_id)
model=AutoModelForSeq2SeqLM.from_pretrained(model_id)

preds=[]
for _,row in df_eval.iterrows():
    pr=(f'Given the question:\n{row["user_query"]}\n\n'
        f'And the retrieved quote:\n"{row["quote"]}" — {row["author"]}\n\n'
        'Is the quote relevant to the question? Answer with "relevant" or "irrelevant".')
    inputs=tokenizer(pr,return_tensors="pt",truncation=True)
    out=model.generate(**inputs,max_new_tokens=5)
    dec=tokenizer.decode(out[0],skip_special_tokens=True).lower()
    preds.append("relevant"if"relevant"in dec else"irrelevant")

y_true=["relevant"if r else"irrelevant"for r in df_eval["is_relevant"]]
print(classification_report(y_true,preds))

# Save cleaned quotes to CSV
df.to_csv("quotes_cleaned.csv",index=False)
print("Saved cleaned data to quotes_cleaned.csv")

# Visualize top authors, top tags, and quote-length distribution
import matplotlib.pyplot as plt
from collections import Counter

df=pd.read_csv("quotes_cleaned.csv")
if isinstance(df['tags'].iloc[0],str):
    df['tags']=df['tags'].apply(eval)

author_counts=df['author'].value_counts().head(10)
plt.figure(figsize=(10,6))
plt.barh(author_counts.index,author_counts.values)
plt.title("Top 10 Authors by Number of Quotes")
plt.xlabel("Number of Quotes")
plt.ylabel("Author")
plt.tight_layout()
plt.show()

all_tags=[tag for sub in df['tags'] for tag in sub]
tag_freq=Counter(all_tags).most_common(10)
tags,counts=zip(*tag_freq)
plt.figure(figsize=(10,6))
plt.barh(tags,counts)
plt.title("Top 10 Most Common Tags")
plt.xlabel("Frequency")
plt.ylabel("Tag")
plt.tight_layout()
plt.show()

df['quote_length']=df['quote'].apply(lambda q:len(q.split()))
plt.figure(figsize=(10,6))
plt.hist(df['quote_length'],bins=20,edgecolor='black')
plt.title("Quote Length Distribution (by Word Count)")
plt.xlabel("Number of Words")
plt.ylabel("Number of Quotes")
plt.tight_layout()
plt.show()

